{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acae67c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T03:04:43.980253Z",
     "iopub.status.busy": "2025-03-20T03:04:43.979849Z",
     "iopub.status.idle": "2025-03-20T03:04:44.004014Z",
     "shell.execute_reply": "2025-03-20T03:04:44.002359Z"
    },
    "papermill": {
     "duration": 0.03151,
     "end_time": "2025-03-20T03:04:44.007105",
     "exception": false,
     "start_time": "2025-03-20T03:04:43.975595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 提取特征的分箱边界\n",
    "# def get_tree_bins(model, feature_index=0):\n",
    "#     tree = model.tree_\n",
    "#     thresholds = tree.threshold[tree.feature == feature_index]\n",
    "#     valid_thresholds = thresholds[thresholds != -2.0]  # 过滤无效节点\n",
    "    \n",
    "#     # 添加数据最小最大值形成完整区间\n",
    "#     min_val = X.iloc[:, feature_index].min()\n",
    "#     max_val = X.iloc[:, feature_index].max()\n",
    "#     bins = np.sort(np.concatenate([[min_val], valid_thresholds, [max_val]]))\n",
    "    \n",
    "#     return np.unique(bins)  # 去重处理\n",
    "\n",
    "# 提取分箱边界（还原实际金额）\n",
    "def get_tree_bins(model,X_Tree, y_Tree, feature_index=0):\n",
    "    import numpy as np\n",
    "    thresholds = model.tree_.threshold[model.tree_.feature == feature_index]\n",
    "    valid_thresholds = thresholds[thresholds != -2.0]\n",
    "    return np.expm1(np.sort(np.concatenate([\n",
    "        [X_Tree.min().values[0]], valid_thresholds, [X_Tree.max().values[0]]\n",
    "    ])))\n",
    "\n",
    "def cast_int(value, default_value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return default_value\n",
    "    \n",
    "def atws_show(in_PCA, in_max_depth, in_min_samples_split, in_min_samples_leaf, in_kde, in_element, in_stat) :\n",
    "    import pandas as pd\n",
    "    \n",
    "    print(\"====================1. 数据获取与加载\")\n",
    "    print(\"====================1.1 加载数据\")\n",
    "    # 加载信用卡欺诈数据集（CSV格式）\n",
    "    data = pd.read_csv('/kaggle/input/abnormal-transaction-warning-system/creditcard.csv')\n",
    "    # 查看数据概况\n",
    "    # print(data.head())\n",
    "    print(f\"数据加载完毕 正常交易: {data['Class'].value_counts()[0]} 条\")\n",
    "    print(f\"数据加载完毕 异常交易: {data['Class'].value_counts()[1]} 条\")\n",
    "    \n",
    "    \n",
    "    print(\"====================2. 数据探索与预处理\")\n",
    "    print(\"====================2.1 处理不平衡数据\")\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    # ​分离特征与标签\n",
    "    X = data.drop('Class', axis=1)  # 特征（V1-V28、Amount、Time）\n",
    "    y = data['Class']               # 标签（0=正常，1=欺诈）\n",
    "    print(f\"正常交易: {y.value_counts()[0]} 条\")\n",
    "    print(f\"异常交易: {y.value_counts()[1]} 条\")\n",
    "    # ​标准化特征​（避免量纲差异影响采样效果）\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # ​分离多数类与少数类\n",
    "    # 正常交易（多数类）\n",
    "    normal_indices = y[y == 0].index\n",
    "    normal_X = X_scaled[y == 0]\n",
    "    # 异常交易（少数类）\n",
    "    fraud_indices = y[y == 1].index\n",
    "    fraud_X = X_scaled[y == 1]\n",
    "    \n",
    "    # ​使用SMOTE生成合成样本\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    fraud_X_resampled, fraud_y_resampled = smote.fit_resample(fraud_X, fraud_indices)\n",
    "    print(\"采样后 样本数：\",(len(fraud_X_resampled) + len(normal_X)))\n",
    "    print(\"采样后 正常比：%.2f%% \" % (len(normal_X) / (len(normal_X) + len(fraud_X_resampled))*100))\n",
    "    print(\"采样后 异常比：%.2f%% \" % (len(fraud_X_resampled) / (len(normal_X) + len(fraud_X_resampled))*100))\n",
    "    \n",
    "    # ​合并数据集\n",
    "    # 合并多数类原始样本与少数类合成样本\n",
    "    import numpy as np\n",
    "    X_combined = np.concatenate([normal_X, fraud_X_resampled], axis=0)\n",
    "    y_combined = np.concatenate([np.zeros(len(normal_X)), np.ones(len(fraud_X_resampled))], axis=0)\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    data = pd.DataFrame(X_combined, columns=X.columns)\n",
    "    data['Class'] = y_combined\n",
    "    \n",
    "    \n",
    "    print(\"====================2.2 特征标准化\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"====================3. 特征工程\")\n",
    "    print(\"====================3.1 基于领域知识的特征生成\")\n",
    "    # 生成时间窗口特征（如小时段）\n",
    "    data['Hour'] = data['Time'] // 3600   # Time单位为秒\n",
    "    # 生成交易频率特征（如用户近期交易次数）\n",
    "    # 假设数据中有UserID列（实际需根据数据集调整）\n",
    "    data['RecentTxCount'] = data.groupby('UserID')['TransactionID'].transform('count')   # 分组聚合 *transform 前一个如果是多维注意lammdba是否返回Series\n",
    "   \n",
    "    if in_PCA == \"要\" :        \n",
    "        print(\"====================3.2 降维\")\n",
    "        from sklearn.decomposition import PCA\n",
    "        \n",
    "        # 对高维特征（V1-V28）进行PCA降维\n",
    "        # 若0<n_components<1，则n_components的值为主成分方差的阈值； 通过设置该变量，即可调整主成分数量K；\n",
    "        # 若n_components≥1，则降维后的特征数为n_components；\n",
    "        # whiten：参数为bool型，是否对降维后的数据的每个特征进行归一化，默认是False\n",
    "        pca = PCA(n_components=28)\n",
    "        # fit(X,y=None) ：用训练数据X训练模型，由于PCA是无监督降维，因此y=None。\n",
    "        # transform(X,y=None) ：训练好模型后，对输入数据X进行降维。\n",
    "        # fit_transform(X) ：用训练数据X训练模型，并对X进行降维。相当于先用fit(X)，再用transform(X)。\n",
    "        # inverse_transform(X) ：将降维后的数据还原成原始数据的近似。(PCA的重建)\n",
    "        pca_features = pca.fit_transform(data[[f'V{i}' for i in range(1, 29)]])  \n",
    "        \n",
    "        # 将降维结果合并到数据集\n",
    "        data_pca = pd.DataFrame(pca_features, columns=[f'PC{i}' for i in range(1, 29)])  # i的范围 1- （n_components+1）\n",
    "        data_pca.to_csv('data_pca.csv',columns=[f'PC{i}' for i in range(1, 29)])   #保存降维过的文件，以便本地对比\n",
    "        data = pd.concat([data, data_pca], axis=1)  # axis= 0(Y轴：默认)/1(x轴)    \n",
    "     \n",
    "    print(\"====================4. 构建最终数据集\")\n",
    "    print(\"====================4.1 分割训练集与测试集\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = data.drop('Class', axis=1)\n",
    "    y = data['Class']\n",
    "    \n",
    "    # 分层抽样保持类别比例\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    print(\"====================4.2 保存数据集\")\n",
    "    # 保存为CSV或Pickle\n",
    "    X_train.to_csv('train_features.csv', index=False)\n",
    "    y_train.to_csv('train_labels.csv', index=False)\n",
    "    X_test.to_csv('test_features.csv', index=False)\n",
    "    y_test.to_csv('test_labels.csv', index=False)\n",
    "    \n",
    "    print(\"====================5. 验证数据集质量\")\n",
    "    print(\"====================5.1 可视化特征分布\")\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure()  # 创建新画布\n",
    "    # 绘制正常与异常交易的金额分布对比\n",
    "    # rug ： 在轴上显示数据点分布（需配合kde使用）\n",
    "    # cumulative ： 是否绘制累积分布图\t\n",
    "    # binwidth ： 单独指定柱宽而非数量\t\n",
    "    # 训练决策树模型（通过max_depth控制分箱数量）\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    regressor = DecisionTreeRegressor(\n",
    "        max_depth=cast_int(in_max_depth,10), #银行业:10-15, 电商:5-8, 保险:8-12\n",
    "        min_samples_split=cast_int(in_min_samples_split,50), #银行业:50-100, 电商:20-50\n",
    "        min_samples_leaf=cast_int(in_min_samples_leaf,20) #统一建议:10-20\n",
    "    )\n",
    "    # 针对右偏分布，进行对数变换增强决策树分箱效果\n",
    "    data['log_amount'] = np.log1p(data['Amount'])    \n",
    "    # 以异常标识Class为目标变量训练决策树\n",
    "    X_Tree = data[['log_amount']]\n",
    "    y_Tree = data['Class']\n",
    "    # 生成示例数据（假设X为特征，y为目标变量）\n",
    "    regressor.fit(X_Tree, y_Tree)       \n",
    "    # 获取分箱边界\n",
    "    tmp_bins = get_tree_bins(regressor,X_Tree, y_Tree)  \n",
    "    print(\"决策树分箱边界:\", tmp_bins.round(2))\n",
    "    sns.histplot(data=data, \n",
    "                 x='RecentTxCount',  # RecentTxCount：交易频率特征  ； Amount：金额\n",
    "                 hue='Class',# 通过label区分正常/异常[1,2]\n",
    "                 element=in_element,  # 柱形样式（bars：默认、step：阶梯状、poly：多边形）\n",
    "                 kde=in_kde,  #启用核密度估计，直观展示概率密度 \n",
    "                 bins=tmp_bins.round(2),#控制分箱数量，建议根据数据量调整（默认自动计算）。数字越大粒度越小\n",
    "                 stat=in_stat, #统计量类型（count:默认、density:归一化、probability:概率 , percent:报告的可读性）\n",
    "                 palette=[\"skyblue\", \"red\"]  ) #自定义颜色\n",
    "    plt.title('Normal vs Fraud Transaction Amount')\n",
    "    # plt.show()\n",
    "    \n",
    "    return fig\n",
    "    \n",
    "    # print(\"====================5.2 特征相关性分析\")\n",
    "    # # 计算特征相关性矩阵\n",
    "    # corr_matrix = data.corr()\n",
    "    # # 绘制热力图\n",
    "    # plt.figure(figsize=(12, 8))\n",
    "    # sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
    "    # plt.title('Feature Correlation Matrix')\n",
    "    # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11321226,
     "sourceId": 95031,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.69008,
   "end_time": "2025-03-20T03:04:44.530944",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-20T03:04:40.840864",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
