{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":95031,"databundleVersionId":11321226,"sourceType":"competition"},{"sourceId":10947219,"sourceType":"datasetVersion","datasetId":6809133}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# init\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimport sys\nsys.path.append(\"/kaggle/input/functions/\")\nfrom FileObject import print_file_info  \n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_path=os.path.join(dirname, filename)\n        print_file_info(file_path)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Main**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nprint(\"====================1. 数据获取与加载\")\nprint(\"====================1.1 加载数据\")\n# 加载信用卡欺诈数据集（CSV格式）\ndata = pd.read_csv('/kaggle/input/abnormal-transaction-warning-system/creditcard.csv')\n# 查看数据概况\n# print(data.head())\nprint(f\"正常交易: {data['Class'].value_counts()[0]} 条\")\nprint(f\"异常交易: {data['Class'].value_counts()[1]} 条\")\n\nprint(\"====================2. 数据探索与预处理\")\nprint(\"====================2.1 处理不平衡数据\")\nfrom sklearn.utils import resample\n\n# 分离多数类（正常）和少数类（异常）\ndf_majority = data[data['Class'] == 0]\ndf_minority = data[data['Class'] == 1]\n\n# 上采样（过采样）少数类（可选方法：SMOTE）\n#resample 数据重采样的工具函数，对数组或稀疏矩阵进行  上采样（过采样）​ 和 ​下采样（欠采样）\ndf_minority_upsampled = resample(df_minority,   #目标数组或稀疏矩阵\n                                replace=True,     # 是否允许重复采样（上采样时需设为 True）\n                                n_samples=len(df_majority),  # 目标样本数量（下采样时需小于原数据量，上采样时需大于原数据量）\n                                random_state=42)  #随机种子（初始化一个伪随机数生成器），42：确保结果可复现\n                                                 #stratify  按目标比例分层采样（适用于分类任务）\n\n# 合并数据集\ndf_balanced = pd.concat([df_majority, df_minority_upsampled])\n\nprint(\"====================2.2 特征标准化\")\nfrom sklearn.preprocessing import StandardScaler\n\n# 标准化金额特征（Amount）\nscaler = StandardScaler()  \n# 计算均值和标准差（fit:mean+std） + 标准正态分布（transform）\n\n# print(\"array_Amount_values 行:%r \" % (len(data['Amount'].values.astype(int))))\n# print(\"array_Amount_values.reshape(-1, 1) 行:%r \" % (len(data['Amount'].values.reshape(-1, 1).astype(int))))\n\ndata['Amount'] = scaler.fit_transform(data['Amount'].values.reshape(-1, 1))     #reshape(行，列) 当行/列不定时，可以设位 -1\n\n# 删除无关列（如时间戳Time）\n# data.drop(['Time'], axis=1, inplace=True)                                       #<<<<<<<<<<<<<<< 导致下一个命令错误\n\nprint(\"====================3. 特征工程\")\nprint(\"====================3.1 基于领域知识的特征生成\")\n# 生成时间窗口特征（如小时段）\ndata['Hour'] = data['Time'] // 3600 % 24  # 假设Time单位为秒\n\n# 生成交易频率特征（如用户近期交易次数）\n# 假设数据中有UserID列（实际需根据数据集调整）\ndata['RecentTxCount'] = data.groupby('UserID')['TransactionID'].transform('count')   # 分组聚合 *transform 前一个如果是多维注意lammdba是否返回Series\n\nprint(\"====================3.2 降维（可选）\")\nfrom sklearn.decomposition import PCA\n\n# 对高维特征（如V1-V28）进行PCA降维\n# 若0<n_components<1，则n_components的值为主成分方差的阈值； 通过设置该变量，即可调整主成分数量K；\n# 若n_components≥1，则降维后的特征数为n_components；\n# whiten：参数为bool型，是否对降维后的数据的每个特征进行归一化，默认是False\npca = PCA(n_components=10)\n# fit(X,y=None) ：用训练数据X训练模型，由于PCA是无监督降维，因此y=None。\n# transform(X,y=None) ：训练好模型后，对输入数据X进行降维。\n# fit_transform(X) ：用训练数据X训练模型，并对X进行降维。相当于先用fit(X)，再用transform(X)。\n# inverse_transform(X) ：将降维后的数据还原成原始数据的近似。(PCA的重建)\npca_features = pca.fit_transform(data[['V1', 'V2', 'V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27', 'V28']])  \n\n# 将降维结果合并到数据集\ndata_pca = pd.DataFrame(pca_features, columns=[f'PC{i}' for i in range(1, 11)])\ndata = pd.concat([data, data_pca], axis=1)  # axis= 0(Y轴：默认)/1(x轴)\n\nprint(\"====================4. 构建最终数据集\")\nprint(\"====================4.1 分割训练集与测试集\")\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop('Class', axis=1)\ny = data['Class']\n\n# 分层抽样保持类别比例\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\nprint(\"====================4.2 保存数据集\")\n# 保存为CSV或Pickle\nX_train.to_csv('train_features.csv', index=False)\ny_train.to_csv('train_labels.csv', index=False)\nX_test.to_csv('test_features.csv', index=False)\ny_test.to_csv('test_labels.csv', index=False)\n\nprint(\"====================5. 验证数据集质量\")\nprint(\"====================5.1 可视化特征分布\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 绘制正常与异常交易的金额分布对比\nsns.histplot(data=data, x='Amount', hue='Class', element='step', stat='density')\nplt.title('Normal vs Fraud Transaction Amount')\nplt.show()\nprint(\"====================5.2 特征相关性分析\")\n# 计算特征相关性矩阵\ncorr_matrix = data.corr()\n\n# 绘制热力图\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\nplt.title('Feature Correlation Matrix')\nplt.show()\n\nprint(\"====================6. 扩展：合成数据生成（可选）\")\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_synth, y_synth = smote.fit_resample(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test 1\n# import os\n\n# # 检查文件路径\n# file_path = \"/kaggle/input/functions/FileObject.py\"\n# if os.path.exists(file_path):\n#     print(\"文件存在！\")\n# else:\n#     print(\"文件不存在，请检查数据集上传路径。\")\n    \n# Test 2\nimport pandas as pd\n\nd1 = [[\"xiaolei\", 20, 10081], [\"xiaowu\", 30, 10082]]\nt1 = pd.DataFrame(d1)\nprint(t1)\n\nd2 = [[\"xiaowang\", 22, 10083], [\"xiaoming\", 25, 10084]]\nt2 = pd.DataFrame(d2)\nprint(t2)\n\n# 默认是上下堆叠\nt = pd.concat([t1, t2])\nprint(t)\n# 左右拼接\nt = pd.concat([t1, t2], axis=1)\nprint(t)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**原版SMOTE的局限性**\n\n* 噪声敏感：若少数类样本本身存在异常值，生成的合成样本可能引入噪声。\n* 边界模糊：在多数类与少数类交界区域生成样本，可能造成分类边界错误。\n\n\n**改良算法**\n\n* Borderline SMOTE\n1. 原理：仅对边界样本（同时被多数类和少数类近邻包围的样本）生成合成样本，避免噪声干扰。\n2. 适用场景：医学影像分类、**金融反欺诈**中的临界样本处理。\n* ADASYN（Adaptive Synthetic Sampling）​\n1. 原理：根据样本邻域中多数类的密度生成合成样本，密度越高生成越多，解决类别不平衡中的“硬”边界问题。\n2. 适用场景：高维数据（如基因表达数据）、复杂分布的非平衡数据。","metadata":{}},{"cell_type":"code","source":"# SMOTE + Tomek Links\n# ​流程：\n#   使用SMOTE生成合成样本；\n#   通过Tomek Links移除多数类与少数类邻域重叠的噪声点，清晰分类边界。\n# ​优势：\n#   减少过采样引入的噪声，提升模型泛化能力。\nfrom imblearn.combine import SMOTETomek\nsmote_tomek = SMOTETomek(random_state=42)\nX_resampled, y_resampled = smote_tomek.fit_resample(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 组合采样（SMOTEENN）​\n# ​流程：\n#   使用SMOTE生成合成样本；\n#   通过ENN（Edited Nearest Neighbours）移除多数类噪声点。\n# ​优势：\n#   同时处理过采样和欠采样，简化流程\nfrom imblearn.combine import SMOTEENN\nsmote_enn = SMOTEENN(random_state=42)\nX_resampled, y_resampled = smote_enn.fit_resample(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SMOTE + ADASYN\n# ​流程：\n#   使用ADASYN生成合成样本（基于邻域密度）；\n#   结合SMOTE进一步补充样本多样性。\n# ​优势：\n#   自适应生成样本，适用于高维、非线性可分数据。\nfrom imblearn.combine import SMOTEENN\nsmote_enn = SMOTEENN(random_state=42)\nX_resampled, y_resampled = smote_enn.fit_resample(X, y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* 方差     是各数据偏离平均值 差值的平方和 的平均数\n* 标准差   是方差的平方根\n* 均方误差（MSE）是各数据偏离真实值 差值的平方和 的平均数","metadata":{}},{"cell_type":"code","source":"# StandardScaler原理\nfrom sklearn.preprocessing import StandardScaler  # 标准化工具\nimport numpy as np\n \nx_np = np.array([[1.5, -1., 2.],\n                [2., 0., 0.]])\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_np)\nprint('矩阵初值为：{}'.format(x_np))\nprint('该矩阵的均值为：{}\\n 该矩阵的标准差为：{}'.format(scaler.mean_,np.sqrt(scaler.var_)))\nprint('标准差标准化的矩阵为：{}'.format(x_train))\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# 创建一个 3 x 2 的矩阵\nX = np.array([[1, 2], [3, 4], [5, 6]])\n\n# 使用 fit_transform 方法标准化数据\nscaler = StandardScaler()\nX_scaled1 = scaler.fit_transform(X)\n\n# 使用 fit 和 transform 方法标准化数据\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled2 = scaler.transform(X)\n\n# 打印结果\nprint(\"使用 fit_transform 方法标准化的结果：\\n\", X_scaled1)\nprint(\"使用 fit 和 transform 方法标准化的结果：\\n\", X_scaled2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# 创建一个 3 x 2 的矩阵\nX = np.array([[1, 2], [3, 4], [5, 6]])\n\n# 创建 StandardScaler 对象\nscaler = StandardScaler()\n\n# 计算均值和标准差\nscaler.fit(X)\n\n# 将数据标准化为标准正态分布\nX_scaled = scaler.transform(X)\n\n# 打印结果\nprint(\"原始数据：\\n\", X)\nprint(\"均值：\", scaler.mean_)\nprint(\"标准差：\", scaler.scale_)\nprint(\"标准化后的数据：\\n\", X_scaled)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 创建一个DataFrame\ndf = pd.DataFrame({\n'Weight': [45, 88, 56, 15, 71],\n'Name': ['Sam', 'Andrea', 'Alex', 'Robin', 'Kia'],\n'Age': [14, 25, 55, 8, 21]\n})\nprint(df)\n# 使用.values属性获取NumPy数组\ndata_array = df.values\n\n# 输出NumPy数组\nprint(data_array)\n\n# 输出NumPy数组 行 +列\nprint(len(data_array))\nprint(len(data_array[0]))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#DataFrame.transform\nimport pandas as pd\n\n# ​1. 按列进行特征工程\n\n# 创建示例数据\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [10, 20, 30]\n})\n\n# 定义函数：将每列乘以 100\ndef multiply_by_100(col):\n    return col + 10\n\n# 对每列应用函数\ndf_transformed = df.transform(multiply_by_100)\nprint(df_transformed)\n\n# # ​2. 生成新特征（基于多列计算）​\n\n# # 创建新列 \"C\" = A + B\n# df['C'] = df.import pandas as pd\n\n# # 创建示例数据\n# df = pd.DataFrame({\n#     'Category': ['A', 'A', 'B', 'B', 'C', 'C'],\n#     'Value': [10, 20, 30, 40, 50, 60]\n# })\n\n# # 使用 lambda 按列生成新特征（例如：分类标签编码）\n# df['Encoded_Category'] = df['Category'].transform(\n#     lambda x: {'A': 0, 'B': 1, 'C': 2}[x]\n# )\n\n# print(df)(lambda x:x['A']+x['B'], axis=0)\n\n# print(df['C'] )\n# sys.exit()\n# # ​3. 标准化数据（Z-Score）\n# from sklearn.preprocessing import StandardScaler\n\n# # 对数值列标准化\n# df[['A', 'B']] = df[['A', 'B']].transform(\n#     lambda x: (x - x.mean()) / x.std(),\n#     axis=0\n# )\n\n# ​4. 分组聚合（GroupBy + Transform）​\n\n# 按某列分组，计算每组的统计量（如均值、中位数）\ndf = pd.DataFrame({\n    'Group': ['A', 'A', 'A', 'B', 'B', 'B'],\n    'Value': [1, 2, 3, 4, 5, 6]\n})\n\n# 计算每组内的均值并填充到原数据\ndf['Group_Mean'] = df.groupby('Group')['Value'].transform('mean')   # 优先使用python内置函数 mean()、std()\nprint(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# 测试数据\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\ndef complex_func(x):\n    x['C'] = x['A'] + x['B']\n    return x['C']  # 返回 Series，但索引可能错乱\n    \n\n# df['C'] = df.transform(lambda x:x['A']+x['B'] if x['A'] % 2 == 1 else x['A'] * x['B'] )\nprint(df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}